# Emotion Recognition

Una soluciÃ³n en Python para detectar la emociÃ³n predominante en un fragmento de audio de menos de 60 segundos combinando:

* **TranscripciÃ³n automÃ¡tica** con Whisper (OpenAI) y traducciÃ³n a inglÃ©s.
* **AnÃ¡lisis de emociones en texto** usando un modelo BERT preentrenado.
* **ValidaciÃ³n y refuerzo** con un LLM (ChatGPT) para decidir la emociÃ³n final e incluir un emoji representativo.

---

## ğŸ“‹ Estructura del proyecto

```
EMOTION-RECOGNITION/
â”œâ”€â”€ audio_data/           # Audios de ejemplo (.wav)
â”‚   â”œâ”€â”€ alegria.wav
â”‚   â”œâ”€â”€ asco.wav
â”‚   â””â”€â”€ ...              
â”œâ”€â”€ text_data/            # Transcripciones de audio (.txt)
â”‚   â”œâ”€â”€ alegria.txt
â”‚   â”œâ”€â”€ asco.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ transcribe.py         # Script de transcripciÃ³n con Whisper
â”œâ”€â”€ BERT.py               # AnÃ¡lisis de emociones de texto con Transformers
â”œâ”€â”€ prosodic.py           # (Opcional) extracciÃ³n de caracterÃ­sticas prosÃ³dicas
â”œâ”€â”€ main.py               # Orquestador: transcribe â†’ analiza texto â†’ refuerza con LLM
â”œâ”€â”€ requirements.txt      # Dependencias del proyecto
â””â”€â”€ README.md             # DocumentaciÃ³n (este archivo)
```

---

## ğŸ”§ Requisitos

* Python 3.8 o superior
* ffmpeg (para que Whisper procese audio)
* Variables de entorno:

  * `API_KEY`: tu clave de OpenAI para acceder al servicio de ChatGPT.

### Dependencias Python

Se incluye un `requirements.txt`; instala con:

```bash
pip install -r requirements.txt
```

**Principales librerÃ­as**:

* `openai-whisper`  : TranscripciÃ³n y traducciÃ³n de audio.
* `transformers`    : Modelos BERT para clasificaciÃ³n de texto.
* `openai`         : Cliente para invocar el LLM (ChatGPT).
* `pyAudioAnalysis` : (Opcional) extracciÃ³n de caracterÃ­sticas acÃºsticas.
* `pydub`          : Manejo de archivos de audio.

---

## ğŸš€ InstalaciÃ³n

1. Clona el repositorio:

   ```bash
   git clone https://github.com/socratesmb/emotion-recognition.git 
   cd emotion-recognition
   ```

2. Crea y activa un entorno virtual (recomendado):
   
    ```bash
    python -m venv venv
    source venv/bin/activate   # Linux/macOS
    venv\\Scripts\\activate  # Windows
    ```

3. Instala dependencias:

   ```bash
   pip install -r requirements.txt
   ```


4. AsegÃºrate de tener `ffmpeg` en tu PATH:
   ```bash
    # Linux/macOS
    sudo apt-get install ffmpeg
    # Windows
    Descargar desde https://ffmpeg.org/download.html y configurar variable PATH
    ```

---

## ğŸ–¥ï¸ Uso

### 1. TranscripciÃ³n (`transcribe.py`)

```bash
python transcribe.py
```

* Edita `audio_path` en `transcribe.py` para apuntar al archivo `.wav` que quieras procesar.
* Genera en `text_data/` un `.txt` con la transcripciÃ³n traducida a inglÃ©s.

### 2. AnÃ¡lisis de texto (`BERT.py`)

```bash
python BERT.py
```

* Edita `file_path` dentro de `BERT.py` para apuntar al `.txt` deseado.
* Muestra en consola un diccionario con puntuaciones de emociones.

### 3. Pipeline completo (`main.py`)

```bash
python main.py
```

* Modifica la variable `audio` al inicio de `main.py` para seleccionar tu audio.
* El script:

  1. Transcribe y traduce.
  2. Analiza emociones de texto.
  3. EnvÃ­a un prompt al LLM.
  4. Imprime la emociÃ³n predominante con emoji.

---

## ğŸ“‚ Estructura detallada

* **`audio_data/`**: ejemplos de archivos `.wav` con cada emociÃ³n.
* **`text_data/`**: transcripciones resultantes.
* **`transcribe.py`**: funciÃ³n `transcribe_audio` basada en whisper.
* **`BERT.py`**: funciÃ³n `analyze_text_emotion` y lector de archivos.
* **`prosodic.py`**: extracciÃ³n de caracterÃ­sticas acÃºsticas (opcional).
* **`main.py`**: orquestador principal combinando todo.
* **`requirements.txt`**: lista de paquetes pip.

---

## ğŸ¤ Contribuciones

Â¡Bienvenidas! Para aportar:

1. Haz un *fork* del proyecto.
2. Crea una rama con tu mejora: `git checkout -b feature/mi-mejora`.
3. Realiza tus cambios y haz *commit*: `git commit -m "Agrega ..."`.
4. Empuja tu rama: `git push origin feature/mi-mejora`.
5. Abre un *Pull Request*.

---

## âš–ï¸ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.
